{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 747207812235079101\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4943878553\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 440094980914651047\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import multi_gpu_model\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create YOLOv3 model with 9 anchors and 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\saving.py:1009: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((1, 1, 1024, 18) vs (255, 1024, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\saving.py:1009: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((18,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\saving.py:1009: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((1, 1, 512, 18) vs (255, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\saving.py:1009: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((18,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\saving.py:1009: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((1, 1, 256, 18) vs (255, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\python3_27\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\saving.py:1009: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((18,) vs (255,)).\n",
      "  weight_values[i].shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights model_data/yolo.h5.\n",
      "Freeze the first 185 layers of total 252 layers.\n",
      "use the multi_gpu_model for model training \n",
      "the gpu-model compile is ok!!\n",
      "Train on 924 samples, val on 162 samples, with batch size 16.\n",
      "Epoch 1/500\n",
      "57/57 [==============================] - 87s 2s/step - loss: 25.1893 - val_loss: 15.7237\n",
      "Epoch 2/500\n",
      "57/57 [==============================] - 72s 1s/step - loss: 14.6581 - val_loss: 14.4497\n",
      "Epoch 3/500\n",
      "57/57 [==============================] - 69s 1s/step - loss: 13.7881 - val_loss: 13.9215\n",
      "Epoch 4/500\n",
      "57/57 [==============================] - 68s 1s/step - loss: 13.2801 - val_loss: 13.3514\n",
      "Epoch 5/500\n",
      "57/57 [==============================] - 69s 1s/step - loss: 12.9616 - val_loss: 13.0587\n",
      "Epoch 6/500\n",
      "57/57 [==============================] - 68s 1s/step - loss: 12.7678 - val_loss: 12.5050\n",
      "Epoch 7/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 12.5386 - val_loss: 12.2219\n",
      "Epoch 8/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 12.2499 - val_loss: 12.5860\n",
      "Epoch 9/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 12.0212 - val_loss: 12.1991\n",
      "Epoch 10/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 11.9439 - val_loss: 16.9795\n",
      "Epoch 11/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 11.8393 - val_loss: 11.8907\n",
      "Epoch 12/500\n",
      "57/57 [==============================] - 67s 1s/step - loss: 11.7820 - val_loss: 11.8914\n",
      "Epoch 13/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 11.6184 - val_loss: 11.6662\n",
      "Epoch 14/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 11.5659 - val_loss: 11.4336\n",
      "Epoch 15/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 11.3685 - val_loss: 11.5807\n",
      "Epoch 16/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 11.4461 - val_loss: 11.3412\n",
      "Epoch 17/500\n",
      "57/57 [==============================] - 70s 1s/step - loss: 11.1782 - val_loss: 11.3753\n",
      "Epoch 18/500\n",
      "57/57 [==============================] - 69s 1s/step - loss: 11.3165 - val_loss: 11.4900\n",
      "Epoch 19/500\n",
      "57/57 [==============================] - 70s 1s/step - loss: 11.1911 - val_loss: 11.2660\n",
      "Epoch 20/500\n",
      "57/57 [==============================] - 70s 1s/step - loss: 11.3199 - val_loss: 11.2733\n",
      "Epoch 21/500\n",
      "57/57 [==============================] - 71s 1s/step - loss: 11.2793 - val_loss: 10.7785\n",
      "Epoch 22/500\n",
      "57/57 [==============================] - 69s 1s/step - loss: 11.0718 - val_loss: 11.2623\n",
      "Epoch 23/500\n",
      "57/57 [==============================] - 68s 1s/step - loss: 11.1610 - val_loss: 11.5383\n",
      "Epoch 24/500\n",
      "57/57 [==============================] - 69s 1s/step - loss: 11.2966 - val_loss: 120.7884\n",
      "Epoch 25/500\n",
      "57/57 [==============================] - 67s 1s/step - loss: 11.6678 - val_loss: 11.6706\n",
      "Epoch 26/500\n",
      "57/57 [==============================] - 67s 1s/step - loss: 11.4602 - val_loss: 11.4223\n",
      "Epoch 27/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 11.1930 - val_loss: 11.2091\n",
      "Epoch 28/500\n",
      "57/57 [==============================] - 69s 1s/step - loss: 11.0904 - val_loss: 11.2551\n",
      "Epoch 29/500\n",
      "57/57 [==============================] - 67s 1s/step - loss: 11.1057 - val_loss: 11.5165\n",
      "Epoch 30/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.9654 - val_loss: 11.2128\n",
      "Epoch 31/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 11.0388 - val_loss: 11.2391\n",
      "Epoch 32/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.8383 - val_loss: 10.9653\n",
      "Epoch 33/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.7978 - val_loss: 11.1254\n",
      "Epoch 34/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.9314 - val_loss: 11.0782\n",
      "Epoch 35/500\n",
      "57/57 [==============================] - 69s 1s/step - loss: 10.8220 - val_loss: 10.9450\n",
      "Epoch 36/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.8898 - val_loss: 10.9927\n",
      "Epoch 37/500\n",
      "57/57 [==============================] - 67s 1s/step - loss: 10.7797 - val_loss: 10.8270\n",
      "Epoch 38/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.8638 - val_loss: 11.0186\n",
      "Epoch 39/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.6974 - val_loss: 10.7613\n",
      "Epoch 40/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.6793 - val_loss: 10.9308\n",
      "Epoch 41/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.8744 - val_loss: 11.0298\n",
      "Epoch 42/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.6788 - val_loss: 10.5792\n",
      "Epoch 43/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.8647 - val_loss: 10.8767\n",
      "Epoch 44/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.6798 - val_loss: 11.0230\n",
      "Epoch 45/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.6899 - val_loss: 10.5123\n",
      "Epoch 46/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.7126 - val_loss: 10.7911\n",
      "Epoch 47/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5794 - val_loss: 11.0460\n",
      "Epoch 48/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.8048 - val_loss: 11.0599\n",
      "Epoch 49/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.7612 - val_loss: 10.9713\n",
      "Epoch 50/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.5389 - val_loss: 10.7807\n",
      "Epoch 51/500\n",
      "57/57 [==============================] - 67s 1s/step - loss: 10.7020 - val_loss: 10.7773\n",
      "Epoch 52/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.7180 - val_loss: 11.0047\n",
      "Epoch 53/500\n",
      "57/57 [==============================] - 67s 1s/step - loss: 10.6711 - val_loss: 11.1517\n",
      "Epoch 54/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.6917 - val_loss: 10.7249\n",
      "Epoch 55/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5339 - val_loss: 10.9582\n",
      "Epoch 56/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.7638 - val_loss: 11.0733\n",
      "Epoch 57/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.7612 - val_loss: 10.8237\n",
      "Epoch 58/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.7429 - val_loss: 10.8554\n",
      "Epoch 59/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.5296 - val_loss: 10.7243\n",
      "Epoch 60/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5252 - val_loss: 10.9691\n",
      "Epoch 61/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.6146 - val_loss: 10.8319\n",
      "Epoch 62/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5853 - val_loss: 10.6695\n",
      "Epoch 63/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.6806 - val_loss: 10.7636\n",
      "Epoch 64/500\n",
      "57/57 [==============================] - 67s 1s/step - loss: 10.7606 - val_loss: 10.8914\n",
      "Epoch 65/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.7768 - val_loss: 10.7330\n",
      "Epoch 66/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5337 - val_loss: 10.6250\n",
      "Epoch 67/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.6130 - val_loss: 10.5433\n",
      "Epoch 68/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5080 - val_loss: 10.9798\n",
      "Epoch 69/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.6608 - val_loss: 11.0419\n",
      "Epoch 70/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.6327 - val_loss: 10.8114\n",
      "Epoch 71/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.6878 - val_loss: 10.5105\n",
      "Epoch 72/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4532 - val_loss: 11.0363\n",
      "Epoch 73/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5619 - val_loss: 10.7805\n",
      "Epoch 74/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.6305 - val_loss: 10.6880\n",
      "Epoch 75/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5081 - val_loss: 10.6101\n",
      "Epoch 76/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.5885 - val_loss: 10.9104\n",
      "Epoch 77/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.6528 - val_loss: 10.6647\n",
      "Epoch 78/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.5473 - val_loss: 10.5228\n",
      "Epoch 79/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5055 - val_loss: 10.7927\n",
      "Epoch 80/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5781 - val_loss: 10.6724\n",
      "Epoch 81/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.5603 - val_loss: 10.6680\n",
      "Epoch 82/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5181 - val_loss: 10.4943\n",
      "Epoch 83/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.5124 - val_loss: 10.7060\n",
      "Epoch 84/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.4711 - val_loss: 10.5457\n",
      "Epoch 85/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4473 - val_loss: 10.6223\n",
      "Epoch 86/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5120 - val_loss: 10.9842\n",
      "Epoch 87/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4935 - val_loss: 10.4398\n",
      "Epoch 88/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.5249 - val_loss: 10.4157\n",
      "Epoch 89/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.4587 - val_loss: 11.0375\n",
      "Epoch 90/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4285 - val_loss: 10.5582\n",
      "Epoch 91/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.4311 - val_loss: 10.6119\n",
      "Epoch 92/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.5280 - val_loss: 10.5099\n",
      "Epoch 93/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.4729 - val_loss: 10.6363\n",
      "Epoch 94/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.5534 - val_loss: 10.9342\n",
      "Epoch 95/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5696 - val_loss: 10.5596\n",
      "Epoch 96/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.5113 - val_loss: 10.7776\n",
      "Epoch 97/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5174 - val_loss: 10.6933\n",
      "Epoch 98/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4468 - val_loss: 10.7751\n",
      "Epoch 99/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4863 - val_loss: 10.3648\n",
      "Epoch 100/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.3390 - val_loss: 10.7234\n",
      "Epoch 101/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.5478 - val_loss: 10.6630\n",
      "Epoch 102/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.3118 - val_loss: 10.6282\n",
      "Epoch 103/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4775 - val_loss: 10.8155\n",
      "Epoch 104/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5757 - val_loss: 10.7145\n",
      "Epoch 105/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.4259 - val_loss: 10.3493\n",
      "Epoch 106/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.4942 - val_loss: 10.5653\n",
      "Epoch 107/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4830 - val_loss: 10.4283\n",
      "Epoch 108/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.3362 - val_loss: 10.4854\n",
      "Epoch 109/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4530 - val_loss: 10.8010\n",
      "Epoch 110/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.3454 - val_loss: 10.4149\n",
      "Epoch 111/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.3694 - val_loss: 10.8253\n",
      "Epoch 112/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.3889 - val_loss: 10.4411\n",
      "Epoch 113/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5299 - val_loss: 10.8200\n",
      "Epoch 114/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.3247 - val_loss: 11.0217\n",
      "Epoch 115/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5037 - val_loss: 10.6248\n",
      "Epoch 116/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4504 - val_loss: 10.5666\n",
      "Epoch 117/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5026 - val_loss: 10.7781\n",
      "Epoch 118/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.7499 - val_loss: 11.3568\n",
      "Epoch 119/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.7010 - val_loss: 10.7587\n",
      "Epoch 120/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4491 - val_loss: 10.5871\n",
      "Epoch 121/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4217 - val_loss: 10.2281\n",
      "Epoch 122/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.4586 - val_loss: 10.6764\n",
      "Epoch 123/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4746 - val_loss: 10.9485\n",
      "Epoch 124/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.4210 - val_loss: 10.4268\n",
      "Epoch 125/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.2986 - val_loss: 10.7221\n",
      "Epoch 126/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.4281 - val_loss: 10.5675\n",
      "Epoch 127/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.3982 - val_loss: 10.7239\n",
      "Epoch 128/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.3039 - val_loss: 10.5149\n",
      "Epoch 129/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4607 - val_loss: 10.6041\n",
      "Epoch 130/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.4236 - val_loss: 10.3930\n",
      "Epoch 131/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4929 - val_loss: 10.7034\n",
      "Epoch 132/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.3588 - val_loss: 10.4553\n",
      "Epoch 133/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.2824 - val_loss: 10.4530\n",
      "Epoch 134/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.4484 - val_loss: 10.4834\n",
      "Epoch 135/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.3104 - val_loss: 10.5409\n",
      "Epoch 136/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4564 - val_loss: 10.3165\n",
      "Epoch 137/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.2217 - val_loss: 10.6412\n",
      "Epoch 138/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.2937 - val_loss: 10.5957\n",
      "Epoch 139/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.4084 - val_loss: 10.5710\n",
      "Epoch 140/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.3412 - val_loss: 10.4140\n",
      "Epoch 141/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.5247 - val_loss: 11.3586\n",
      "Epoch 142/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.6187 - val_loss: 10.5346\n",
      "Epoch 143/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.3838 - val_loss: 10.3986\n",
      "Epoch 144/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.4679 - val_loss: 10.7835\n",
      "Epoch 145/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.3693 - val_loss: 10.7012\n",
      "Epoch 146/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.3481 - val_loss: 10.3448\n",
      "Epoch 147/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.3041 - val_loss: 10.5093\n",
      "Epoch 148/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.3218 - val_loss: 10.4067\n",
      "Epoch 149/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.4247 - val_loss: 10.6518\n",
      "Epoch 150/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.2667 - val_loss: 10.7366\n",
      "Epoch 151/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.3495 - val_loss: 10.4840\n",
      "Epoch 152/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.2154 - val_loss: 10.3486\n",
      "Epoch 153/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.2030 - val_loss: 10.2684\n",
      "Epoch 154/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1901 - val_loss: 10.4937\n",
      "Epoch 155/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1745 - val_loss: 10.3015\n",
      "Epoch 156/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1581 - val_loss: 10.5064\n",
      "Epoch 157/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0510 - val_loss: 10.1024\n",
      "Epoch 158/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1827 - val_loss: 10.3452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0895 - val_loss: 10.1755\n",
      "Epoch 160/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1961 - val_loss: 10.2354\n",
      "Epoch 161/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.2596 - val_loss: 10.1888\n",
      "Epoch 162/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1567 - val_loss: 10.5763\n",
      "Epoch 163/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0260 - val_loss: 10.3055\n",
      "Epoch 164/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1194 - val_loss: 10.3086\n",
      "Epoch 165/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1455 - val_loss: 10.1143\n",
      "Epoch 166/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0760 - val_loss: 10.4354\n",
      "Epoch 167/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.1515 - val_loss: 10.1655\n",
      "Epoch 168/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0856 - val_loss: 10.0021\n",
      "Epoch 169/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1098 - val_loss: 10.3684\n",
      "Epoch 170/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0780 - val_loss: 10.0399\n",
      "Epoch 171/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.1273 - val_loss: 9.9449\n",
      "Epoch 172/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1079 - val_loss: 10.4168\n",
      "Epoch 173/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0621 - val_loss: 10.1497\n",
      "Epoch 174/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0825 - val_loss: 10.1215\n",
      "Epoch 175/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0781 - val_loss: 10.1920\n",
      "Epoch 176/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.1178 - val_loss: 10.3122\n",
      "Epoch 177/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1198 - val_loss: 10.4430\n",
      "Epoch 178/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0268 - val_loss: 10.0665\n",
      "Epoch 179/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1352 - val_loss: 10.1356\n",
      "Epoch 180/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0880 - val_loss: 10.2176\n",
      "Epoch 181/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0568 - val_loss: 10.1817\n",
      "Epoch 182/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1856 - val_loss: 10.1451\n",
      "Epoch 183/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9873 - val_loss: 10.1946\n",
      "Epoch 184/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0273 - val_loss: 10.3232\n",
      "Epoch 185/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1155 - val_loss: 10.3267\n",
      "Epoch 186/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1374 - val_loss: 10.2154\n",
      "Epoch 187/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0639 - val_loss: 10.3108\n",
      "Epoch 188/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9951 - val_loss: 9.8746\n",
      "Epoch 189/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.1187 - val_loss: 10.2852\n",
      "Epoch 190/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0930 - val_loss: 10.1095\n",
      "Epoch 191/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0371 - val_loss: 10.4079\n",
      "Epoch 192/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9403 - val_loss: 10.1136\n",
      "Epoch 193/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1777 - val_loss: 10.3633\n",
      "Epoch 194/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9412 - val_loss: 10.0628\n",
      "Epoch 195/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1571 - val_loss: 10.3224\n",
      "Epoch 196/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9937 - val_loss: 10.1823\n",
      "Epoch 197/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0080 - val_loss: 10.0985\n",
      "Epoch 198/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0837 - val_loss: 10.1496\n",
      "Epoch 199/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0411 - val_loss: 9.9351\n",
      "Epoch 200/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0223 - val_loss: 10.2031\n",
      "Epoch 201/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1181 - val_loss: 10.0417\n",
      "Epoch 202/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0339 - val_loss: 10.5025\n",
      "Epoch 203/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1532 - val_loss: 10.0855\n",
      "Epoch 204/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9992 - val_loss: 10.1658\n",
      "Epoch 205/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1957 - val_loss: 10.3670\n",
      "Epoch 206/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0157 - val_loss: 10.0646\n",
      "Epoch 207/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1752 - val_loss: 9.9390\n",
      "Epoch 208/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0589 - val_loss: 10.2309\n",
      "Epoch 209/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1219 - val_loss: 10.1491\n",
      "Epoch 210/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0112 - val_loss: 10.1481\n",
      "Epoch 211/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0618 - val_loss: 10.2785\n",
      "Epoch 212/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0206 - val_loss: 10.0957\n",
      "Epoch 213/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0254 - val_loss: 10.5143\n",
      "Epoch 214/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.2307 - val_loss: 10.1165\n",
      "Epoch 215/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9701 - val_loss: 10.1904\n",
      "Epoch 216/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1465 - val_loss: 10.4424\n",
      "Epoch 217/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9640 - val_loss: 9.8306\n",
      "Epoch 218/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0913 - val_loss: 10.3136\n",
      "Epoch 219/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1080 - val_loss: 10.2492\n",
      "Epoch 220/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0732 - val_loss: 10.1903\n",
      "Epoch 221/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0508 - val_loss: 10.1761\n",
      "Epoch 222/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9889 - val_loss: 10.3446\n",
      "Epoch 223/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1370 - val_loss: 10.1687\n",
      "Epoch 224/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0318 - val_loss: 10.1175\n",
      "Epoch 225/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0591 - val_loss: 10.1180\n",
      "Epoch 226/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0427 - val_loss: 10.4386\n",
      "Epoch 227/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0616 - val_loss: 9.8788\n",
      "Epoch 228/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1258 - val_loss: 10.5671\n",
      "Epoch 229/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9915 - val_loss: 10.3455\n",
      "Epoch 230/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0372 - val_loss: 10.1220\n",
      "Epoch 231/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0233 - val_loss: 10.2152\n",
      "Epoch 232/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0293 - val_loss: 10.2128\n",
      "Epoch 233/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0065 - val_loss: 10.2582\n",
      "Epoch 234/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0458 - val_loss: 10.1334\n",
      "Epoch 235/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0558 - val_loss: 10.1846\n",
      "Epoch 236/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0249 - val_loss: 10.1518\n",
      "Epoch 237/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0223 - val_loss: 10.2416\n",
      "Epoch 238/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1049 - val_loss: 10.3899\n",
      "Epoch 239/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0087 - val_loss: 10.1830\n",
      "Epoch 240/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0395 - val_loss: 9.9676\n",
      "Epoch 241/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0292 - val_loss: 10.0410\n",
      "Epoch 242/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1613 - val_loss: 10.5667\n",
      "Epoch 243/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0141 - val_loss: 10.1578\n",
      "Epoch 244/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0500 - val_loss: 10.2845\n",
      "Epoch 245/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9693 - val_loss: 10.1403\n",
      "Epoch 246/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0289 - val_loss: 10.2657\n",
      "Epoch 247/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0328 - val_loss: 10.2464\n",
      "Epoch 248/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1564 - val_loss: 10.1744\n",
      "Epoch 249/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9783 - val_loss: 10.0117\n",
      "Epoch 250/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0234 - val_loss: 10.3734\n",
      "Epoch 251/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0592 - val_loss: 10.3594\n",
      "Epoch 252/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9872 - val_loss: 10.1105\n",
      "Epoch 253/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1251 - val_loss: 10.0630\n",
      "Epoch 254/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0518 - val_loss: 10.2200\n",
      "Epoch 255/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0552 - val_loss: 10.1617\n",
      "Epoch 256/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9634 - val_loss: 10.2154\n",
      "Epoch 257/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0455 - val_loss: 10.1548\n",
      "Epoch 258/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9402 - val_loss: 10.3698\n",
      "Epoch 259/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.2265 - val_loss: 10.1107\n",
      "Epoch 260/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9798 - val_loss: 10.2670\n",
      "Epoch 261/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1157 - val_loss: 10.1291\n",
      "Epoch 262/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9669 - val_loss: 10.1831\n",
      "Epoch 263/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0934 - val_loss: 10.2318\n",
      "Epoch 264/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9516 - val_loss: 10.2544\n",
      "Epoch 265/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1334 - val_loss: 10.0896\n",
      "Epoch 266/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9757 - val_loss: 10.0612\n",
      "Epoch 267/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1352 - val_loss: 10.1807\n",
      "Epoch 268/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9636 - val_loss: 10.2430\n",
      "Epoch 269/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9834 - val_loss: 10.2225\n",
      "Epoch 270/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.1367 - val_loss: 10.2988\n",
      "Epoch 271/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9778 - val_loss: 10.0780\n",
      "Epoch 272/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0768 - val_loss: 10.1768\n",
      "Epoch 273/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9634 - val_loss: 10.3453\n",
      "Epoch 274/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0553 - val_loss: 10.1733\n",
      "Epoch 275/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0653 - val_loss: 10.0593\n",
      "Epoch 276/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0919 - val_loss: 9.9128\n",
      "Epoch 277/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9293 - val_loss: 10.2205\n",
      "Epoch 278/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9987 - val_loss: 10.2773\n",
      "Epoch 279/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0772 - val_loss: 10.2281\n",
      "Epoch 280/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9822 - val_loss: 10.0449\n",
      "Epoch 281/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0239 - val_loss: 10.0671\n",
      "Epoch 282/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1088 - val_loss: 10.1911\n",
      "Epoch 283/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0253 - val_loss: 10.1270\n",
      "Epoch 284/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9286 - val_loss: 10.1991\n",
      "Epoch 285/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1895 - val_loss: 10.2278\n",
      "Epoch 286/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9480 - val_loss: 9.7100\n",
      "Epoch 287/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1004 - val_loss: 10.1967\n",
      "Epoch 288/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9208 - val_loss: 10.4961\n",
      "Epoch 289/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0888 - val_loss: 10.2711\n",
      "Epoch 290/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0303 - val_loss: 10.3093\n",
      "Epoch 291/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9550 - val_loss: 10.2152\n",
      "Epoch 292/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0971 - val_loss: 10.4068\n",
      "Epoch 293/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0410 - val_loss: 10.1431\n",
      "Epoch 294/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0769 - val_loss: 10.1201\n",
      "Epoch 295/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9630 - val_loss: 10.0477\n",
      "Epoch 296/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0529 - val_loss: 10.3039\n",
      "Epoch 297/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0404 - val_loss: 10.3219\n",
      "Epoch 298/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.8895 - val_loss: 10.0416\n",
      "Epoch 299/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9993 - val_loss: 10.4119\n",
      "Epoch 300/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0112 - val_loss: 10.1985\n",
      "Epoch 301/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9979 - val_loss: 10.1763\n",
      "Epoch 302/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0262 - val_loss: 10.2510\n",
      "Epoch 303/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0115 - val_loss: 10.0733\n",
      "Epoch 304/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9881 - val_loss: 10.1430\n",
      "Epoch 305/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0536 - val_loss: 10.3513\n",
      "Epoch 306/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9570 - val_loss: 10.0911\n",
      "Epoch 307/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0067 - val_loss: 10.2254\n",
      "Epoch 308/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9692 - val_loss: 9.9451\n",
      "Epoch 309/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0497 - val_loss: 10.6164\n",
      "Epoch 310/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0370 - val_loss: 9.8960\n",
      "Epoch 311/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9668 - val_loss: 10.3202\n",
      "Epoch 312/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0401 - val_loss: 10.1082\n",
      "Epoch 313/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9812 - val_loss: 10.2823\n",
      "Epoch 314/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9915 - val_loss: 10.0734\n",
      "Epoch 315/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0395 - val_loss: 10.2234\n",
      "Epoch 316/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9811 - val_loss: 10.2397\n",
      "Epoch 317/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0571 - val_loss: 10.2761\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 64s 1s/step - loss: 10.0359 - val_loss: 10.2127\n",
      "Epoch 319/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0276 - val_loss: 10.0988\n",
      "Epoch 320/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9367 - val_loss: 10.5476\n",
      "Epoch 321/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9381 - val_loss: 10.1104\n",
      "Epoch 322/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1029 - val_loss: 10.1875\n",
      "Epoch 323/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0088 - val_loss: 10.3318\n",
      "Epoch 324/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0022 - val_loss: 10.1352\n",
      "Epoch 325/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.8839 - val_loss: 10.2944\n",
      "Epoch 326/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1381 - val_loss: 10.1948\n",
      "Epoch 327/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9569 - val_loss: 9.9803\n",
      "Epoch 328/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0748 - val_loss: 10.3466\n",
      "Epoch 329/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0200 - val_loss: 9.6818\n",
      "Epoch 330/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0472 - val_loss: 10.2696\n",
      "Epoch 331/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1045 - val_loss: 10.2015\n",
      "Epoch 332/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9027 - val_loss: 10.2033\n",
      "Epoch 333/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0342 - val_loss: 10.0799\n",
      "Epoch 334/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9159 - val_loss: 10.1293\n",
      "Epoch 335/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0129 - val_loss: 10.1616\n",
      "Epoch 336/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0205 - val_loss: 10.2281\n",
      "Epoch 337/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0675 - val_loss: 10.2000\n",
      "Epoch 338/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9169 - val_loss: 10.1345\n",
      "Epoch 339/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9701 - val_loss: 10.3400\n",
      "Epoch 340/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0256 - val_loss: 10.4069\n",
      "Epoch 341/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9402 - val_loss: 10.1715\n",
      "Epoch 342/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0728 - val_loss: 10.1882\n",
      "Epoch 343/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9831 - val_loss: 10.1562\n",
      "Epoch 344/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9590 - val_loss: 10.2089\n",
      "Epoch 345/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0036 - val_loss: 10.2108\n",
      "Epoch 346/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9399 - val_loss: 10.1666\n",
      "Epoch 347/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9273 - val_loss: 10.0799\n",
      "Epoch 348/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.1332 - val_loss: 10.1649\n",
      "Epoch 349/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9363 - val_loss: 10.3457\n",
      "Epoch 350/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0721 - val_loss: 10.1450\n",
      "Epoch 351/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9736 - val_loss: 10.1299\n",
      "Epoch 352/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9742 - val_loss: 10.2996\n",
      "Epoch 353/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.8771 - val_loss: 10.0188\n",
      "Epoch 354/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.1140 - val_loss: 10.0034\n",
      "Epoch 355/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.8960 - val_loss: 10.4982\n",
      "Epoch 356/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0204 - val_loss: 10.2941\n",
      "Epoch 357/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0991 - val_loss: 10.1876\n",
      "Epoch 358/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9147 - val_loss: 10.1116\n",
      "Epoch 359/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0167 - val_loss: 10.2337\n",
      "Epoch 360/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9686 - val_loss: 10.2494\n",
      "Epoch 361/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0673 - val_loss: 10.2678\n",
      "Epoch 362/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0614 - val_loss: 10.1256\n",
      "Epoch 363/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.8919 - val_loss: 10.1377\n",
      "Epoch 364/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0608 - val_loss: 10.1686\n",
      "Epoch 365/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9769 - val_loss: 10.2195\n",
      "Epoch 366/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0680 - val_loss: 9.8831\n",
      "Epoch 367/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9554 - val_loss: 10.4042\n",
      "Epoch 368/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9600 - val_loss: 9.8715\n",
      "Epoch 369/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0000 - val_loss: 10.1692\n",
      "Epoch 370/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0192 - val_loss: 10.2830\n",
      "Epoch 371/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9304 - val_loss: 10.2643\n",
      "Epoch 372/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9459 - val_loss: 10.1532\n",
      "Epoch 373/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0088 - val_loss: 10.1861\n",
      "Epoch 374/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9442 - val_loss: 10.1361\n",
      "Epoch 375/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0253 - val_loss: 10.1107\n",
      "Epoch 376/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9615 - val_loss: 10.4010\n",
      "Epoch 377/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0992 - val_loss: 9.9213\n",
      "Epoch 378/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9580 - val_loss: 10.1534\n",
      "Epoch 379/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9599 - val_loss: 10.1966\n",
      "Epoch 380/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0127 - val_loss: 10.4532\n",
      "Epoch 381/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9771 - val_loss: 10.1639\n",
      "Epoch 382/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9659 - val_loss: 10.2295\n",
      "Epoch 383/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0075 - val_loss: 10.1602\n",
      "Epoch 384/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9919 - val_loss: 10.1652\n",
      "Epoch 385/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0334 - val_loss: 10.3049\n",
      "Epoch 386/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9699 - val_loss: 10.1306\n",
      "Epoch 387/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0034 - val_loss: 10.0029\n",
      "Epoch 388/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9893 - val_loss: 10.0157\n",
      "Epoch 389/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9906 - val_loss: 10.4608\n",
      "Epoch 390/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9360 - val_loss: 10.1919\n",
      "Epoch 391/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0369 - val_loss: 9.9874\n",
      "Epoch 392/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0046 - val_loss: 10.2683\n",
      "Epoch 393/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0538 - val_loss: 10.0259\n",
      "Epoch 394/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9677 - val_loss: 10.1919\n",
      "Epoch 395/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9004 - val_loss: 10.1753\n",
      "Epoch 396/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0829 - val_loss: 10.1682\n",
      "Epoch 397/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9143 - val_loss: 9.9889\n",
      "Epoch 398/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 66s 1s/step - loss: 10.0009 - val_loss: 10.1742\n",
      "Epoch 399/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0227 - val_loss: 10.0995\n",
      "Epoch 400/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9887 - val_loss: 10.1149\n",
      "Epoch 401/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9483 - val_loss: 10.1977\n",
      "Epoch 402/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9004 - val_loss: 10.2717\n",
      "Epoch 403/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9925 - val_loss: 10.0804\n",
      "Epoch 404/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0583 - val_loss: 9.9823\n",
      "Epoch 405/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9881 - val_loss: 10.0312\n",
      "Epoch 406/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9110 - val_loss: 10.1065\n",
      "Epoch 407/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0582 - val_loss: 10.3138\n",
      "Epoch 408/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9462 - val_loss: 10.3629\n",
      "Epoch 409/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0758 - val_loss: 10.0009\n",
      "Epoch 410/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0159 - val_loss: 10.0466\n",
      "Epoch 411/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9025 - val_loss: 10.1711\n",
      "Epoch 412/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0332 - val_loss: 10.1386\n",
      "Epoch 413/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.8470 - val_loss: 10.1068\n",
      "Epoch 414/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0326 - val_loss: 10.2678\n",
      "Epoch 415/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9426 - val_loss: 10.1716\n",
      "Epoch 416/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0831 - val_loss: 10.2364\n",
      "Epoch 417/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9970 - val_loss: 10.2878\n",
      "Epoch 418/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9050 - val_loss: 10.3007\n",
      "Epoch 419/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0125 - val_loss: 10.0851\n",
      "Epoch 420/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9759 - val_loss: 10.2220\n",
      "Epoch 421/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0176 - val_loss: 10.0091\n",
      "Epoch 422/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.8271 - val_loss: 10.0964\n",
      "Epoch 423/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9595 - val_loss: 10.0896\n",
      "Epoch 424/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9332 - val_loss: 10.1984\n",
      "Epoch 425/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9721 - val_loss: 10.3497\n",
      "Epoch 426/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0021 - val_loss: 10.2050\n",
      "Epoch 427/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9872 - val_loss: 10.0368\n",
      "Epoch 428/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.8815 - val_loss: 10.1502\n",
      "Epoch 429/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9694 - val_loss: 10.0626\n",
      "Epoch 430/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9881 - val_loss: 10.2194\n",
      "Epoch 431/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9784 - val_loss: 10.0691\n",
      "Epoch 432/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0128 - val_loss: 10.0440\n",
      "Epoch 433/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9374 - val_loss: 10.0481\n",
      "Epoch 434/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9905 - val_loss: 10.3989\n",
      "Epoch 435/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9643 - val_loss: 10.0132\n",
      "Epoch 436/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9932 - val_loss: 10.0800\n",
      "Epoch 437/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9460 - val_loss: 10.2341\n",
      "Epoch 438/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9627 - val_loss: 10.0290\n",
      "Epoch 439/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9443 - val_loss: 10.1922\n",
      "Epoch 440/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.8809 - val_loss: 10.0346\n",
      "Epoch 441/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9652 - val_loss: 10.1279\n",
      "Epoch 442/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9232 - val_loss: 10.1621\n",
      "Epoch 443/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0052 - val_loss: 10.1927\n",
      "Epoch 444/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9706 - val_loss: 10.1662\n",
      "Epoch 445/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0034 - val_loss: 10.1790\n",
      "Epoch 446/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9063 - val_loss: 10.0167\n",
      "Epoch 447/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9858 - val_loss: 10.2090\n",
      "Epoch 448/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9751 - val_loss: 10.0122\n",
      "Epoch 449/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9315 - val_loss: 9.8980\n",
      "Epoch 450/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0408 - val_loss: 10.4946\n",
      "Epoch 451/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0166 - val_loss: 9.9930\n",
      "Epoch 452/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9020 - val_loss: 10.0961\n",
      "Epoch 453/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 10.0234 - val_loss: 9.9623\n",
      "Epoch 454/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9909 - val_loss: 10.2852\n",
      "Epoch 455/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9490 - val_loss: 10.0457\n",
      "Epoch 456/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9386 - val_loss: 10.1568\n",
      "Epoch 457/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0503 - val_loss: 9.9691\n",
      "Epoch 458/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9131 - val_loss: 9.9134\n",
      "Epoch 459/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9857 - val_loss: 10.3913\n",
      "Epoch 460/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9913 - val_loss: 10.0963\n",
      "Epoch 461/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9527 - val_loss: 10.1709\n",
      "Epoch 462/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9216 - val_loss: 10.1879\n",
      "Epoch 463/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9311 - val_loss: 10.1720\n",
      "Epoch 464/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9632 - val_loss: 10.1599\n",
      "Epoch 465/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9847 - val_loss: 9.7597\n",
      "Epoch 466/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9465 - val_loss: 10.1353\n",
      "Epoch 467/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9392 - val_loss: 10.2163\n",
      "Epoch 468/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0036 - val_loss: 9.7601\n",
      "Epoch 469/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9198 - val_loss: 10.2648\n",
      "Epoch 470/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0157 - val_loss: 10.2687\n",
      "Epoch 471/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9023 - val_loss: 10.2017\n",
      "Epoch 472/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0289 - val_loss: 9.9573\n",
      "Epoch 473/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9320 - val_loss: 10.1788\n",
      "Epoch 474/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9894 - val_loss: 10.1918\n",
      "Epoch 475/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9091 - val_loss: 10.1869\n",
      "Epoch 476/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0249 - val_loss: 10.2564\n",
      "Epoch 477/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9931 - val_loss: 10.2474\n",
      "Epoch 478/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9054 - val_loss: 10.2309\n",
      "Epoch 479/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9819 - val_loss: 9.8188\n",
      "Epoch 480/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0278 - val_loss: 10.3355\n",
      "Epoch 481/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9633 - val_loss: 10.1480\n",
      "Epoch 482/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9183 - val_loss: 10.1593\n",
      "Epoch 483/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0312 - val_loss: 9.9873\n",
      "Epoch 484/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9570 - val_loss: 9.8667\n",
      "Epoch 485/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9679 - val_loss: 10.1554\n",
      "Epoch 486/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9576 - val_loss: 10.2425\n",
      "Epoch 487/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 10.0477 - val_loss: 10.2222\n",
      "Epoch 488/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.8068 - val_loss: 10.1266\n",
      "Epoch 489/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9332 - val_loss: 10.0984\n",
      "Epoch 490/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9441 - val_loss: 10.0530\n",
      "Epoch 491/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9769 - val_loss: 10.3513\n",
      "Epoch 492/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9583 - val_loss: 10.3126\n",
      "Epoch 493/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9705 - val_loss: 10.1897\n",
      "Epoch 494/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 10.0912 - val_loss: 9.9946\n",
      "Epoch 495/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.8922 - val_loss: 10.4052\n",
      "Epoch 496/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9224 - val_loss: 9.9560\n",
      "Epoch 497/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9558 - val_loss: 10.0199\n",
      "Epoch 498/500\n",
      "57/57 [==============================] - 65s 1s/step - loss: 9.9207 - val_loss: 10.0499\n",
      "Epoch 499/500\n",
      "57/57 [==============================] - 66s 1s/step - loss: 9.9408 - val_loss: 10.2113\n",
      "Epoch 500/500\n",
      "57/57 [==============================] - 64s 1s/step - loss: 9.9302 - val_loss: 10.0262\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Lambda, add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "from keras.backend.tensorflow_backend import set_session \n",
    "\n",
    "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from yolo3.utils import get_random_data\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def _main():\n",
    "    \n",
    "    annotation_path = 'data/2007_train.txt'\n",
    "    log_dir = 'logs/'\n",
    "    classes_path = 'model_data/voc_classes.txt'\n",
    "    anchors_path = 'model_data/yolo_anchors.txt'\n",
    "    class_names = get_classes(classes_path)\n",
    "    num_classes = len(class_names)\n",
    "    anchors = get_anchors(anchors_path)\n",
    "\n",
    "    input_shape = (416,416) # multiple of 32, hw\n",
    "    is_tiny_version = len(anchors)==6 # default setting\n",
    "    # use tiny model if need else darkent53 model for train\n",
    "    if is_tiny_version:\n",
    "        \n",
    "         pass\n",
    "        \n",
    "        #with tf.device('/cpu:0'):\n",
    "            #template_model = create_tiny_model(input_shape, anchors, num_classes,\n",
    "                #freeze_body=1, weights_path='model_data/yolo_weights.h5')\n",
    "            #print('model.input shape', model.input_layers)\n",
    "    else:\n",
    "        '''build the darknet53 model on the cpu'''\n",
    "        with tf.device('/cpu:0'):\n",
    "            template_model = create_model(input_shape, anchors, num_classes,\n",
    "                freeze_body=1, weights_path='model_data/yolo.h5') # make sure you know what you freeze\n",
    "#*******************************************************************            \n",
    "            #model_data/yolo.h5\n",
    "            #logs/200200yolo/ep225-loss18.263-val_loss17.069.h5\n",
    "            #logs/200200yolo/wurenji.h5\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    logging = TensorBoard(log_dir=log_dir)\n",
    "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "                                 monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "\n",
    "    # template_model input\n",
    "    # print('model inputs', model.input)\n",
    "    '''add shedule for reduce lr ratio by epoch monitor'''\n",
    "    def shedule(epoch):\n",
    "        if epoch <= 150:\n",
    "            return 1e-3\n",
    "        elif epoch <= 170:\n",
    "            return 1e-4\n",
    "        else:\n",
    "            return 1e-5\n",
    "    \n",
    "    lr_shedule = LearningRateScheduler(shedule)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "    val_split = 0.15\n",
    "    with open(annotation_path) as f:\n",
    "        lines = f.readlines()\n",
    "    np.random.seed(10101)\n",
    "    np.random.shuffle(lines)\n",
    "    np.random.seed(None)\n",
    "    num_val = int(len(lines)*val_split)\n",
    "    num_train = len(lines) - num_val\n",
    "\n",
    "    '''\n",
    "    write test code for multi_gpus\n",
    "    '''\n",
    "    model = template_model\n",
    "    print('use the multi_gpu_model for model training ')\n",
    "\n",
    "    # for layer in template_model.layers:\n",
    "    #     layer.trainable = True\n",
    "    # print(model.summary())\n",
    "\n",
    "    # Train with frozen layers first, to get a stable loss.\n",
    "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
    "    if True:\n",
    "        #print('model inputlayer', template_model.input_layers)\n",
    "        optimizer =Adam(lr=0.01)\n",
    "        #optimizer =Adam(lr=0.01)\n",
    "        #optimizer =SGD(lr=0.001)\n",
    "        #optimizer =SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(optimizer=optimizer, loss={\n",
    "            # use custom yolo_loss Lambda layer.\n",
    "            'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "        print(\"the gpu-model compile is ok!!\")\n",
    "        batch_size = 16\n",
    "        \n",
    "        # model = multi_gpu_model(model, gpus=2)\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        history_logs = model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "                steps_per_epoch=max(1, num_train//batch_size),\n",
    "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "                validation_steps=max(1, num_val//batch_size),\n",
    "                epochs=500,\n",
    "                initial_epoch=0,\n",
    "                # callbacks=[logging, checkpoint, lr_shedule]\n",
    "                callbacks=[logging, lr_shedule, checkpoint])\n",
    "        # wirte loss record to a json file\n",
    "        with codecs.open('logs/jiaotong_1/history_logs.json', 'w', 'utf-8') as outfile:\n",
    "            json.dump(history_logs.history, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    " \n",
    "        '''template_model summary'''\n",
    "        # print(template_model.summary())\n",
    "        '''save the model config and weigths on cpu field share the weights'''\n",
    "        # with tf.device('/cpu:0'):\n",
    "        template_model.save(log_dir+'wurenji.h5')\n",
    "\n",
    "    '''loads_wights of trained_weights_stage1.h5'''\n",
    "    \n",
    "'''functions'''\n",
    "def get_loss_fig():\n",
    "    epochs = 40\n",
    "    data = []\n",
    "    with codecs.open(\"logs/113/history_logs.json\", \"r\", \"utf-8\") as f:\n",
    "        for line in f:\n",
    "            dic = json.loads(line)\n",
    "            data.append(dic)\n",
    "    logs = data[0]\n",
    "    loss = logs['loss']\n",
    "    val_loss = logs['val_loss']\n",
    "    x = [i+1 for i in range(epochs)]\n",
    "    plt.figure(0)\n",
    "    plt.plot(x, loss, 'r-', label='loss')\n",
    "    plt.plot(x, val_loss, 'y-', label='val_loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.savefig('traffic_loss.png')\n",
    "\n",
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path, encoding='utf8') as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "#******************************************************\n",
    "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/yolo.h5'):\n",
    "    '''create the training model'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
    "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
    "\n",
    "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
    "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "        else:\n",
    "            for i in range(len(model_body.layers)):\n",
    "                model_body.layers[i].trainable = True\n",
    "            print('UnFreeze all of the model_body layers.')\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})([*model_body.output, *y_true])\n",
    "        \n",
    "        #([*model_body.output + y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "def _get_available_devices():\n",
    "    return [x.name for x in K.get_session().list_devices()]\n",
    "\n",
    "\n",
    "def _normalize_device_name(name):\n",
    "    name = '/' + ':'.join(name.lower().replace('/', '').split(':')[-2:])\n",
    "    return name\n",
    "\n",
    "'''modify the devices('/cpu:0') concatenate to add for yolo loss tensor(1,)'''\n",
    "\n",
    "\n",
    "def multi_gpu_model(model, gpus=None):\n",
    "    \n",
    "    if K.backend() != 'tensorflow':\n",
    "        raise ValueError('`multi_gpu_model` is only available '\n",
    "                         'with the TensorFlow backend.')\n",
    "\n",
    "    available_devices = _get_available_devices()\n",
    "    available_devices = [_normalize_device_name(name) for name in available_devices]\n",
    "    if not gpus:\n",
    "        # Using all visible GPUs when not specifying `gpus`\n",
    "        # e.g. CUDA_VISIBLE_DEVICES=0,2 python3 keras_mgpu.py\n",
    "        gpus = len([x for x in available_devices if 'gpu' in x])\n",
    "\n",
    "    if isinstance(gpus, (list, tuple)):\n",
    "        if len(gpus) <= 1:\n",
    "            raise ValueError('For multi-gpu usage to be effective, '\n",
    "                             'call `multi_gpu_model` with `len(gpus) >= 2`. '\n",
    "                             'Received: `gpus=%s`' % gpus)\n",
    "        num_gpus = len(gpus)\n",
    "        target_gpu_ids = gpus\n",
    "    else:\n",
    "        if gpus <= 1:\n",
    "            raise ValueError('For multi-gpu usage to be effective, '\n",
    "                             'call `multi_gpu_model` with `gpus >= 2`. '\n",
    "                             'Received: `gpus=%d`' % gpus)\n",
    "        num_gpus = gpus\n",
    "        target_gpu_ids = range(num_gpus)\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    target_devices = ['/cpu:0'] + ['/gpu:%d' % i for i in target_gpu_ids]\n",
    "    for device in target_devices:\n",
    "        if device not in available_devices:\n",
    "            raise ValueError(\n",
    "                'To call `multi_gpu_model` with `gpus=%d`, '\n",
    "                'we expect the following devices to be available: %s. '\n",
    "                'However this machine only has: %s. '\n",
    "                'Try reducing `gpus`.' % (gpus,\n",
    "                                          target_devices,\n",
    "                                          available_devices))\n",
    "\n",
    "    def get_slice(data, i, parts):\n",
    "        shape = tf.shape(data)\n",
    "        batch_size = shape[:1]\n",
    "        input_shape = shape[1:]\n",
    "        step = batch_size // parts\n",
    "        if i == num_gpus - 1:\n",
    "            size = batch_size - step * i\n",
    "        else:\n",
    "            size = step\n",
    "        size = tf.concat([size, input_shape], axis=0)\n",
    "        stride = tf.concat([step, input_shape * 0], axis=0)\n",
    "        start = stride * i\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    all_outputs = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        all_outputs.append([])\n",
    "\n",
    "    # Place a copy of the model on each GPU,\n",
    "    # each getting a slice of the inputs.\n",
    "    for i, gpu_id in enumerate(target_gpu_ids):\n",
    "        with tf.device('/gpu:%d' % gpu_id):\n",
    "            with tf.name_scope('replica_%d' % gpu_id):\n",
    "                inputs = []\n",
    "                # Retrieve a slice of the input.\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_i = Lambda(get_slice,\n",
    "                                     output_shape=input_shape,\n",
    "                                     arguments={'i': i,\n",
    "                                                'parts': num_gpus})(x)\n",
    "                    inputs.append(slice_i)\n",
    "\n",
    "                # Apply model on slice\n",
    "                # (creating a model replica on the target device).\n",
    "                outputs = model(inputs)\n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "\n",
    "                # Save the outputs for merging back together later.\n",
    "                for o in range(len(outputs)):\n",
    "                    all_outputs[o].append(outputs[o])\n",
    "\n",
    "    # Merge outputs on CPU.\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for name, outputs in zip(model.output_names, all_outputs):\n",
    "            merged.append(add(outputs, name=name))\n",
    "        return Model(model.inputs, merged)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # memory of gpu allowcate\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    config.log_device_placement=True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "    \n",
    "    \n",
    "    _main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
