{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageSets\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "會出現四個檔案\n",
    "Main\n",
    "├──test.txt\n",
    "├──train.txt\n",
    "├──trainval.txt\n",
    "├──val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    " \n",
    "trainval_percent = 0.1\n",
    "train_percent = 0.9\n",
    "xmlfilepath = 'data/VOC2007/Annotations'  \n",
    "txtsavepath = 'data/VOC2007/ImageSets\\Main'  \n",
    "total_xml = os.listdir(xmlfilepath)\n",
    " \n",
    "num = len(total_xml)\n",
    "list = range(num)\n",
    "tv = int(num * trainval_percent)\n",
    "tr = int(tv * train_percent)\n",
    "trainval = random.sample(list, tv)\n",
    "train = random.sample(trainval, tr)\n",
    " \n",
    "if not os.path.exists(txtsavepath):\n",
    "    print('not exist...{}'.format(txtsavepath))\n",
    "    os.makedirs(txtsavepath)\n",
    "    \n",
    "ftrainval = open('data/VOC2007/ImageSets/Main/trainval.txt', 'w')  \n",
    "ftest = open('data/VOC2007/ImageSets/Main/test.txt', 'w')  \n",
    "ftrain = open('data/VOC2007/ImageSets/Main/train.txt', 'w')  \n",
    "fval = open('data/VOC2007/ImageSets/Main/val.txt', 'w') \n",
    " \n",
    "for i in list:\n",
    "    name = total_xml[i][:-4] + '\\n'\n",
    "    if i in trainval:\n",
    "        ftrainval.write(name)\n",
    "        if i in train:\n",
    "            ftest.write(name)\n",
    "        else:\n",
    "            fval.write(name)\n",
    "    else:\n",
    "        ftrain.write(name)\n",
    "        \n",
    "ftrainval.close()\n",
    "ftrain.close()\n",
    "fval.close()\n",
    "ftest.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python voc_annotation.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "會出現三個檔案\n",
    "├──2007_test.txt\n",
    "├──2007_train.txt\n",
    "├──2007_val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from os import getcwd\n",
    "\n",
    "sets=[('2007', 'train'), ('2007', 'val'), ('2007', 'test')]\n",
    "\n",
    "classes = [\"方標\"]     #修改为自己的类别\n",
    "\n",
    "def convert_annotation(year, image_id, list_file):\n",
    "    in_file = open('data/VOC%s/Annotations/%s.xml'%(year, image_id),encoding=\"utf-8\")\n",
    "    tree=ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        if cls not in classes or int(difficult)==1:\n",
    "            continue\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n",
    "        list_file.write(\" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id))\n",
    "\n",
    "wd = getcwd()\n",
    "\n",
    "for year, image_set in sets:\n",
    "    image_ids = open('data/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split()\n",
    "    list_file = open('%s_%s.txt'%(year, image_set), 'w',encoding=\"utf-8\")\n",
    "    for image_id in image_ids:\n",
    "        list_file.write('%s/data/VOC%s/JPEGImages/%s.jpg'%(wd, year, image_id))\n",
    "        convert_annotation(year, image_id, list_file)\n",
    "        list_file.write('\\n')\n",
    "    list_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='2007_test.txt' mode='w' encoding='utf-8'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 修改類別文件\n",
    "找到model_data/voc_classes.txt文件，將裡邊的類別修改為自己的類別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 修改anchor文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們知道yolo是利用9個anchor來檢測目標的位置，因此這9個anchor設置的好壞，會直接影響到訓練與檢測效果。\n",
    "\n",
    "我們使用kmeans方法來聚類得到與我們數據集接近的anchors，在keras-yolov3下邊有一個kmeans.py文件，打開後將filename改為2007_train.txt（2處），保存運行該腳本，會得到9個anchors的坐標值以及準確率。記下這9個坐標值，然後打開model_data中的yolo_anchors.txt，按照文本中的格式，將9個anchors值依次寫入即可\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K anchors:\n",
      " [[ 11  11]\n",
      " [ 16  15]\n",
      " [ 24  24]\n",
      " [ 33  36]\n",
      " [ 44  45]\n",
      " [ 59  60]\n",
      " [ 91  91]\n",
      " [147 144]\n",
      " [267 253]]\n",
      "Accuracy: 81.01%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "class YOLO_Kmeans:\n",
    " \n",
    "    def __init__(self, cluster_number, filename):\n",
    "        self.cluster_number = cluster_number\n",
    "        self.filename = \"data/2007_train.txt\"\n",
    " \n",
    "    def iou(self, boxes, clusters):  # 1 box -> k clusters\n",
    "        n = boxes.shape[0]\n",
    "        k = self.cluster_number\n",
    " \n",
    "        box_area = boxes[:, 0] * boxes[:, 1]\n",
    "        box_area = box_area.repeat(k)\n",
    "        box_area = np.reshape(box_area, (n, k))\n",
    " \n",
    "        cluster_area = clusters[:, 0] * clusters[:, 1]\n",
    "        cluster_area = np.tile(cluster_area, [1, n])\n",
    "        cluster_area = np.reshape(cluster_area, (n, k))\n",
    " \n",
    "        box_w_matrix = np.reshape(boxes[:, 0].repeat(k), (n, k))\n",
    "        cluster_w_matrix = np.reshape(np.tile(clusters[:, 0], (1, n)), (n, k))\n",
    "        min_w_matrix = np.minimum(cluster_w_matrix, box_w_matrix)\n",
    " \n",
    "        box_h_matrix = np.reshape(boxes[:, 1].repeat(k), (n, k))\n",
    "        cluster_h_matrix = np.reshape(np.tile(clusters[:, 1], (1, n)), (n, k))\n",
    "        min_h_matrix = np.minimum(cluster_h_matrix, box_h_matrix)\n",
    "        inter_area = np.multiply(min_w_matrix, min_h_matrix)\n",
    " \n",
    "        result = inter_area / (box_area + cluster_area - inter_area)\n",
    "        return result\n",
    " \n",
    "    def avg_iou(self, boxes, clusters):\n",
    "        accuracy = np.mean([np.max(self.iou(boxes, clusters), axis=1)])\n",
    "        return accuracy\n",
    " \n",
    "    def kmeans(self, boxes, k, dist=np.median):\n",
    "        box_number = boxes.shape[0]\n",
    "        distances = np.empty((box_number, k))\n",
    "        last_nearest = np.zeros((box_number,))\n",
    "        np.random.seed()\n",
    "        clusters = boxes[np.random.choice(\n",
    "            box_number, k, replace=False)]  # init k clusters\n",
    "        while True:\n",
    " \n",
    "            distances = 1 - self.iou(boxes, clusters)\n",
    " \n",
    "            current_nearest = np.argmin(distances, axis=1)\n",
    "            if (last_nearest == current_nearest).all():\n",
    "                break  # clusters won't change\n",
    "            for cluster in range(k):\n",
    "                clusters[cluster] = dist(  # update clusters\n",
    "                    boxes[current_nearest == cluster], axis=0)\n",
    " \n",
    "            last_nearest = current_nearest\n",
    " \n",
    "        return clusters\n",
    " \n",
    "    def result2txt(self, data):\n",
    "        f = open(\"yolo_anchors.txt\", 'w')\n",
    "        row = np.shape(data)[0]\n",
    "        for i in range(row):\n",
    "            if i == 0:\n",
    "                x_y = \"%d,%d\" % (data[i][0], data[i][1])\n",
    "            else:\n",
    "                x_y = \", %d,%d\" % (data[i][0], data[i][1])\n",
    "            f.write(x_y)\n",
    "        f.close()\n",
    " \n",
    "    def txt2boxes(self):\n",
    "        f = open(self.filename, 'r')\n",
    "        dataSet = []\n",
    "        for line in f:\n",
    "            infos = line.split(\" \")\n",
    "            length = len(infos)\n",
    "            for i in range(1, length):\n",
    "                width = int(infos[i].split(\",\")[2]) - \\\n",
    "                    int(infos[i].split(\",\")[0])\n",
    "                height = int(infos[i].split(\",\")[3]) - \\\n",
    "                    int(infos[i].split(\",\")[1])\n",
    "                dataSet.append([width, height])\n",
    "        result = np.array(dataSet)\n",
    "        f.close()\n",
    "        return result\n",
    " \n",
    "    def txt2clusters(self):\n",
    "        all_boxes = self.txt2boxes()\n",
    "        result = self.kmeans(all_boxes, k=self.cluster_number)\n",
    "        result = result[np.lexsort(result.T[0, None])]\n",
    "        self.result2txt(result)\n",
    "        print(\"K anchors:\\n {}\".format(result))\n",
    "        print(\"Accuracy: {:.2f}%\".format(\n",
    "            self.avg_iou(all_boxes, result) * 100))\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    cluster_number = 9\n",
    "    filename = \"data/2007_train.txt\"  # This is the file that records all box sizes.\n",
    "    kmeans = YOLO_Kmeans(cluster_number, filename)\n",
    "    kmeans.txt2clusters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
